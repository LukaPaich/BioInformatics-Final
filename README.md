# Free University Bioinformatics Final

### Using finetuning to identify introns/exons/boundaries in chromosomes
### Based on this <a href="https://arxiv.org/pdf/2311.12884">Paper</a>

  In this project we aim to recreate the results of the above mentioned paper in order to accurately predict whether sequences of DNA contain intron/exon boundary regions.
The paper uses a custom deep learning model to achieve very high (~95% F1 score) accuracy on test data, and we aim to achieve that using a different model.

  To be more specific, in the paper they used a custom model built using LSTM and attention to accurately convey the context of the DNA sequences to identify motifs. They also use simple models like CNNs (Convolutional Neural Networks and RNNs (Recurrent Neural Networks) to compare the accuracy of their custom 'DeepDeCode' model. In this project we use a pre-trained model <a href="https://huggingface.co/zhihan1996/DNABERT-2-117M">DNABERT-2</a> to fine-tune it to fit the papers problem and try to match the accuracy and scores of their custom trained model. In addition we also aim to make the fine-tuning require a lot less GPU power and leverage the time spent pre-training an existing model.
  
  In addition, we also rewrote the data gathering and generating pipeline to use modern Python version and be simple to use. The Papers <a href="https://github.com/asmitapoddar/Deep-Learning-DNA-Sequences">Github Repository</a> has a pretty outdated pipeline for gathering and generating datasets for this particular problem. Our reworked version has easier customizability, less required dependencies, and is much easier to use. We suspect that this pipeline can be used for other tasks as well.

## The Model
  As we mentioned, we use DNABERT-2 pre-trained model to fine-tune on this specific downstream task. The code for parsing the dataset, loading the model, and fine-tuning it, is available in this repository as a Jupyter Notebook. The notebook also includes the output of the latest run, where we fine-tuned the model on the dataset generated by our pipeline on 14th Human Chromosome. The last runs F1 score was 0.93, which is very close to the top-performing models 0.95 in the paper. We will include the training loss graph during the training process, the latest run took about 30 minutes to train, with 3 epochs and 14th Chromosome.
  
  ![image](https://github.com/user-attachments/assets/0f6ade7b-2f99-4a36-998c-da9e7467c7a6)

  Judging by the graph, it is trivial to deduce that additional training would not have yielded better results, so the main focus should be to gather more meaningful genetic data with accurate annotations.
  In addition, here are the last runs scores:

  | Accuracy | F1    | Precision | Recall | Loss |
  | -------- | ----- | --------- | ------ | ---- |
  | 0.929    | 0.928 | 0.928     | 0.928  | 0.22 |

  For additional graphs and data regarding the fine-tuning the model, click <a href="https://api.wandb.ai/links/gkldi20-free-university-of-tbilisi/zm9l0dly">here</a>
  
## The Data
  Originally, The Papers repository includes a lot of code and unnecessary dependencies for their data pipeline. We rewrote it from scratch to only use ```pandas``` framework and work with the latest Python version (3.12). The pipeline is easy to modify and could easily be changed to generate datasets for 3 or more classification problem. For example, with one line of change, the dataset will have 3 labels, either exon, intron, or the boundary between the two.

## The Code
  The project is structured into two main Jupyter Notebook files, each serving a distinct purpose in the workflow:

  1. **prepare_dataset.ipynb**: Dedicated to the data preparation phase. It contains code to download and prepare the chromosome data for future training. We ran this notebook locally on our computers

  2. **model.ipynb**: Focuses on the actual fine-tuning/training of the model. Trains on the output of the **prepare_dataset.ipynb**. We ran this notebook on kaggle.

Together, these notebooks encapsulate the end-to-end process of preparing the data and training a model.